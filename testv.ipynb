{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"\n",
    "file_list = os.listdir(data_path)\n",
    "func = lambda  a : a.replace(\".txt\", \"\").split(\"-\")\n",
    "count_size = lambda a : os.stat(a).st_size\n",
    "data = [[func(title)[0], func(title)[1], data_path + title, count_size(data_path + title)] for title in file_list]\n",
    "files = pd.DataFrame(data, columns=['Author', 'Name of The work', 'FilePath', 'FileSize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    delimeters = [\"\\\\n\", \"'\"]\n",
    "    stop_words = []\n",
    "    with open(\"./stopyPL.txt\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word = repr(line)\n",
    "            for c in delimeters:\n",
    "                word = word.replace(c, \"\")\n",
    "            stop_words.append(word)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_word(word):\n",
    "    word = word.lower()\n",
    "    special_char = [\".\", \",\", \"-\", \"?\", \"(\", \")\", \"!\", \"\\\\\", \"\\\"\", \":\", \";\", \"*\"]\n",
    "    for char in special_char:\n",
    "        word = word.replace(char, \"\")\n",
    "    return word\n",
    "\n",
    "# 0 - generete bag with all words, 1 - bag without stopwords, 2 - bag with only stopwords\n",
    "def generate_word_bag(FileName, mode=0):\n",
    "    stop_word = get_stop_words()\n",
    "    word_bag = {}\n",
    "    words_in_bag = 0\n",
    "    with open(FileName, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            words = [normalize_word(word) for word in words]\n",
    "            for word in words:\n",
    "                if word != '':\n",
    "                    if (mode==0) or (mode==1 and not word in stop_word) or (mode==2 and word in stop_word):\n",
    "                        words_in_bag += 1\n",
    "                        if word in word_bag.keys():\n",
    "                            word_bag[word] += 1\n",
    "                        else:\n",
    "                            word_bag[word] = 1\n",
    "    return word_bag   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dics_to_df(dics, labels=None):\n",
    "    dfs = []\n",
    "    for dic in dics:\n",
    "        df = pd.DataFrame(columns=dic.keys())\n",
    "        df.loc[0] = dic.values()\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    df = df.replace(np.nan, 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mode=2):\n",
    "    word_bags = []\n",
    "    index = []\n",
    "    labels = []\n",
    "\n",
    "    for path, name, label in zip(files['FilePath'], files['Name of The work'], files['Author']):\n",
    "        word_bags.append(generate_word_bag(path, mode=mode))\n",
    "        index.append(name)\n",
    "        labels.append(label)\n",
    "        \n",
    "    df = merge_dics_to_df(word_bags, index)\n",
    "    df['label'] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_data(df, proporcja):\n",
    "    opis_ucz, opis_test, dec_ucz, dec_test = train_test_split(df.iloc[:,0:-1], df.iloc[:,-1].astype('category').cat.codes, test_size=proporcja)#, random_state=0)\n",
    "    return {\"opis_ucz\":opis_ucz, \"opis_test\":opis_test, \"dec_ucz\":dec_ucz, \"dec_test\":dec_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    df = data.copy()\n",
    "    df = df.drop(columns=['label'])\n",
    "    nwords = df.shape[1]\n",
    "    nbooks = df.shape[0]\n",
    "    tf = np.empty((nbooks, nwords))\n",
    "    sm = df.sum(axis=1)\n",
    "    for i in range(nbooks):\n",
    "        tf[i,:] = np.array(df.iloc[i,: ] / sm[i])\n",
    "    inbooks = np.array((df > 0)*1).sum(axis=0)\n",
    "    idf = [math.log(nbooks / val, 10) for val in inbooks]\n",
    "    tfidf = pd.DataFrame((tf*np.array([idf,]*nbooks)*100), columns=df.columns)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_data(df, proporcja):\n",
    "    opis_ucz, opis_test, dec_ucz, dec_test = train_test_split(df.iloc[:,0:-1], df.iloc[:,-1].astype('category').cat.codes, test_size=proporcja)#, random_state=0)\n",
    "    return {\"opis_ucz\":opis_ucz, \"opis_test\":opis_test, \"dec_ucz\":dec_ucz, \"dec_test\":dec_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weryfikuj(model,dane,show=True):\n",
    "    model.fit(dane[\"opis_ucz\"], dane[\"dec_ucz\"])\n",
    "    wynik_ucz = model.predict(dane[\"opis_ucz\"])\n",
    "    wynik_test = model.predict(dane[\"opis_test\"])\n",
    "\n",
    "    learn_s = model.score(dane['opis_ucz'], dane['dec_ucz'])\n",
    "    test_s = model.score(dane['opis_test'], dane['dec_test'])\n",
    "    \n",
    "    if show:\n",
    "        print(\"\\tWynik dla danych uczących: \", end=\"\")\n",
    "        print(learn_s)\n",
    "        print(\"\\tWynik dla danych testowych: \", end=\"\")\n",
    "        print(test_s)\n",
    "    \n",
    "    return learn_s, test_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    models = {\"NS\": KNeighborsClassifier(n_neighbors=1), \n",
    "                \"KNS\" : KNeighborsClassifier(n_neighbors=5),\n",
    "                \"NP\" : NearestCentroid(),\n",
    "                \"BK\" : GaussianNB(),\n",
    "                \"DT\" : tree.DecisionTreeClassifier(max_depth=5)}\n",
    "\n",
    "    names = [\"NS\", \"KNS\", \"NP\", \"BK\", \"DT\"]\n",
    "    for name in names:\n",
    "        yield models[name]\n",
    "    yield None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dla modelu: KNeighborsClassifier(n_neighbors=1)\n",
      "    Dla danych uczących : 1.0\n",
      "    Dla danych testowych: 0.9444444444444434\n",
      "Dla modelu: KNeighborsClassifier()\n",
      "    Dla danych uczących : 1.0\n",
      "    Dla danych testowych: 0.93111111111111\n",
      "Dla modelu: NearestCentroid()\n",
      "    Dla danych uczących : 1.0\n",
      "    Dla danych testowych: 0.9388888888888879\n",
      "Dla modelu: GaussianNB()\n",
      "    Dla danych uczących : 1.0\n",
      "    Dla danych testowych: 0.9366666666666658\n",
      "Dla modelu: DecisionTreeClassifier(max_depth=5)\n",
      "    Dla danych uczących : 1.0\n",
      "    Dla danych testowych: 0.9277777777777765\n"
     ]
    }
   ],
   "source": [
    "tries = 100\n",
    "df = tfidf(load_data(mode=2))\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "for mod in get_model():\n",
    "    if mod is None:\n",
    "        break\n",
    "    sum_t = 0\n",
    "    sum_l = 0\n",
    "    for i in range(tries):\n",
    "        data = get_learning_data(df, 0.2)\n",
    "        d_l, d_t= weryfikuj(model, data,show=False)\n",
    "        sum_l += d_l\n",
    "        sum_t += d_t\n",
    "    print(\"Dla modelu: \" + str(mod))\n",
    "    print(\"    Dla danych uczących : \" + str(sum_l/tries))\n",
    "    print(\"    Dla danych testowych: \" + str(sum_t/tries))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8dd5b33a86443fe513679c02cd28a92e41225ac53964ff0e7afc55cbcc9ed7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
